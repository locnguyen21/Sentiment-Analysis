import codecs
import re
import unidecode
import string
from Readfile import *
from underthesea import *
#todo
# 1. ÄÆ°a vá» viáº¿t thÆ°á»ng khÃ´ng viáº¿t hoa
# 2. Loáº¡i bá» khoáº£ng tráº¯ng
# 3. Loáº¡i bá» sá»‘
# 4. Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
# 5. Loáº¡i bá» email
# 6. Loáº¡i bá» stop words
# 7. Loáº¡i bá» NAN
# 8. Loáº¡i bá» weblinks
# 9. Expand contractions (if possible not necessary)
# 10. Tokenize tiáº¿ng viá»‡t
# 11. Loáº¡i bá» cÃ¡c cÃ¢u giá»‘ng nhau.

def Filetolist(sourcepath):
    with codecs.open(sourcepath, 'r', encoding='utf-8') as f:
        li = f.readlines()

    review = list()
    for row in li:
        if (row != "\n"):
            review.append(row)
    return review

stopword2 = ["bá»‹", "bá»Ÿi", "cáº£", "cÃ¡c", "cÃ¡i", "cáº§n", "cÃ ng", "chá»‰", "chiáº¿c", "cho",
            "chá»©", "chÆ°a", "chuyá»‡n", "cÃ³", "cÃ³ thá»ƒ", "cá»©", "cá»§a", "cÃ¹ng", "cÅ©ng",
            "Ä‘Ã£", "Ä‘ang", "Ä‘Ã¢y", "Ä‘á»ƒ", "Ä‘áº¿n_ná»—i", "Ä‘á»u", "Ä‘iá»u", "do", "Ä‘Ã³",
            "Ä‘Æ°á»£c", "dÆ°á»›i", "gÃ¬", "khÃ´ng", "khi", "lÃ ", "láº¡i", "lÃªn", "lÃºc", "mÃ ",
            "má»—i", "má»™t_cÃ¡ch", "nÃ y", "nÃªn", "náº¿u", "ngay", "nhiá»u", "nhÆ°", "nhÆ°ng",
            "nhá»¯ng", "nÆ¡i", "nÆ¡i", "ná»¯a", "pháº£i", "qua", "ra", "ráº±ng", "ráº¥t", "rá»“i",
            "sau", "sáº½", "so", "sá»±", "táº¡i", "theo", "thÃ¬", "trÃªn", "trÆ°á»›c", "tá»«",
            "tá»«ng", "vÃ ", "váº«n", "vÃ o", "váº­y", "vÃ¬", "viá»‡c", "vá»›i", "vá»«a"]

stopword = ["ráº±ng", "thÃ¬", "lÃ ", "mÃ "]
punctuation = ['!', '"', '#', '$', '%', '&',"'", '(', ')', '*', '+', ',', '-', '.', '/',
               ':', ';', '<', '=', '>', '?', '@', '[', ']', '^', '_', '~', '`', '{', '|', '}']

# stopword_puctuation = stopword + punctuation
#print(stopword_puctuation)

#todo Loáº¡i bá» ngÃ y thÃ¡ng, email, url
def RelgularExpression(sentence):
    datetime = '\d{1,2}\s?[:/-]\s?\d{1,2}\s?[:/-]\s?\d{4}' \
               '|\d{1,2}\s?[:/-]\s?\d{4}' \
               '|\d{1,2}\s?[:/-]\s?\d{1,2}' \
               '|\d{4}' \
               '/(0[1-9]|1[012])[- \/.](0[1-9]|[12][0-9]|3[01])[- \/.](19|20)\d\d/'\


    email = '[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+'
    url = 'https?:\/\/[^\s]*'
    url2 = r'https?://[^\s<>"]+|www\.[^\s<>"]+'
    num_punctuation = 0
    punctuation1 = r'[!#"$%&()*+,-./:;<=>?@[\]^_`{|}~]'
    punctuation2 = r"[']"
    # cam_than = r'([^a-zA-Z])\1+'
    #cam_than = r'(\D)\1+'
    re_datetime = re.compile(datetime)
    re_email = re.compile(email)
    re_url = re.compile(url)
    re_punc = re.compile(punctuation1)
    match_punc = re.findall(punctuation1, sentence)
    if match_punc:
        num_punctuation = len(match_punc)
    match_punc2 = re.findall(punctuation2,sentence)
    if match_punc2:
        num_punctuation = num_punctuation + len(match_punc2)
    # new_row = list()
    # for row in frame:
    # matchdatetime = re.findall(datetime, sentence)
    # match_email = re.findall(email, sentence)
    # match_url1 = re.findall(url, sentence)
    # match_camthan = re.findall(cam_than, sentence)
    #     #match_url2 = re.findall(url2, row)
    # if matchdatetime:
    #     print(matchdatetime)
    #
    # if match_email:
    #     print(match_email)
    #
    # if match_url1:
    #     print(match_url1)
    #     # if match_url2:
    #     #     print(match_url2)
    #
    #
    # if match_camthan:
    #     print(match_camthan)

    sentence = re.sub(re_datetime,r' date ', sentence)
    sentence = re.sub(re_email, r' email ', sentence)
    sentence = re.sub(re_url,r' url ', sentence)
    #sentence = re.sub(r'[^\w\s]','    ',sentence)
    sentence = re.sub(r'[!"#$%&()*+,-./:;<=>?@[\]^_`{|}~]', r' ', sentence)
    # sentence = re.sub(r'["]', r' ', sentence)
    sentence = re.sub(r"[']", r' ', sentence)
    # sentence = re.sub(r'([A-z])\1+', r'\1', sentence)
    # sentence = ' '.join(sentence.split())
    # loáº¡i bá» cÃ¡c kÃ­ tá»± space thá»«a, cÃ¡c tá»« cáº£m thÃ¡n dÃ i nhÆ° ngonnnnnnnn -> ngon, hayyy -> hay
    # sentence = re.sub(r'(\D)\1+', r'\1', sentence)
    # sentence = re.sub(re_camthan,' ', sentence)
        # new_row.append(row)
    return sentence, num_punctuation

#todo Loáº¡i bá» stopword
def RemovePunc(sentence):
    vn_char_low = r"áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°"
    vn_char_up = r"áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ"
    non_special_char = re.compile(
        r"[^A-Za-z0-9 áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ]+")

    punctuation = re.compile(r"[!#$%&'()*+,-./:;<=>?@[\]^_`{|}~]")
    return re.sub(non_special_char, ' ', sentence)

    # sentence = sentence.split()
    # # print(sentence)
    # new_sentence = []
    # for word in sentence:
    #     if word not in punctuation:
    #         new_sentence.append(word)
    # return ' '.join(new_sentence)

# new_frame = RelgularExpression(review)
# daugach = re.compile(r"[_]")
# tÃªn = r"dáº¥u_gáº¡ch"
# a = re.sub(daugach, ' ', tÃªn)
# print("Ä‘Ã£ xong")
# print(review)
# new_frame2 = RelgularExpression(new_frame)

# print(new_frame2)
# r = u' \xa0\n'
# a = re.sub(r,"",review[0])
# print(a)
# # print(review)
# # print(review[0])
# # print(review[1])
# b = re.sub(r,"",review[1])
# print(b)
# newlist = [a,b]
# print(newlist)
# def RegularExpression()

# word = r"    ÄÃ´ng thÃ¬ ráº¥tttt   Ä‘áº¹p trai  nÃªn lÃ  xinh gÃ¡i he    he   "
# word1 = r"Äang xÃ i MX1. DÃ¹nggggggg bÃ¬nh thÆ°á»nnng ngon, pinnnnn trÃ¢u"
# word2 = r"Má»—i tá»™i   tháº±ng cÃ¹ng dÃ¹ng mÃ  ngá»“i má»™t chá»— thÃ¬ cÃ  giá»±t cÃ  tang"
# # .strip()
# a = (re.sub(r'(\D)\1+', r'\1', word1))
# print(a)

emoji = {
    'â¤': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜‹': ' positive ', 'ğŸ˜†': ' positive ' , 'ğŸ˜¡': ' negative ', 'â™¥': ' positive ', 'ğŸ˜': ' positive ',
    'ğŸ’‹' : ' positive ', 'ğŸ‰' : ' positive ', 'ğŸ’¯' : ' positive ', 'ğŸ‘ğŸ»' : ' positive ', 'ğŸ˜' : ' positive ',
    'ğŸŒº': ' positive ', 'ğŸ˜—': ' positive ', 'ğŸ’š' : ' positive ', 'ğŸ’•': ' positive ',
    'ğŸ˜Š': ' positive ', 'ğŸ˜­': ' negative ', 'ğŸ‘': ' negative ', 'ğŸ™„': ' positive ', 'ğŸ‘ŒğŸ»': ' positive ', ':<': ' negative ', 'ğŸ‘¶': ' positive ',
    'â˜º': ' positive ', 'ğŸ˜“': ' negative ', 'ğŸ˜‚': ' positive ', 'ğŸ‘': ' positive ', 'âœ¨': ' positive ',  'ğŸ˜': ' positive ',
    'ğŸ™': ' negative ', 'ğŸ˜ ': ' negative ', 'ğŸ¤£': ' positive ', 'ğŸ˜³': ' negative ', 'ğŸ˜¢': ' negative ', 'ğŸ˜„': ' positive ', 'ğŸ˜…': ' positive ', 'ğŸ˜Œ': ' positive ',
    'ğŸ˜¤': ' negative ', 'â­': ' positive ', 'â˜¹' : ' negative ','ğŸ˜‘': ' negative ', 'ğŸ˜£': ' negative ', 'ğŸ‘': ' positive ', 'ğŸ˜ˆ': ' negative ', 'ğŸ˜€': ' positive ', 'ğŸ˜‰': ' positive ',
    'ğŸ–’': ' positive ', 'ğŸ‘Œ': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ’ªğŸ»': ' positive ', 'ğŸ˜©': ' positive ', 'ğŸ’“': ' positive ', 'ğŸ˜¥':' negative ',
    'ğŸ’Ÿ': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ˜’': ' negative ',  'ğŸŒŸ': ' positive ', 'ğŸ˜¶': ' negative ', 'ğŸ¤—': ' positive ',
    'ğŸ˜¯': ' negative ', 'ğŸ¤”': ' negative ', 'âœŒ': ' positive', 'ğŸ˜': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜œ': ' positive ', 'ğŸ¤‘':' positive ', 'ğŸ˜š': ' positive ',
    'ğŸ¤¨': ' negative ',  'ğŸ˜¬': ' negative ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜«': ' negative ', 'ğŸ˜»': ' positive ', 'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ',
    'ğŸ¤¤': ' positive ', 'ğŸ˜”': ' negative ', 'ğŸ˜': ' negative ' , 'ğŸ˜ª': ' negative ', 'ğŸ˜§': ' negative ', 'ğŸ˜±': ' negative ', 'ğŸ˜°':' nagative ', 'ğŸ¤­': ' positive ',
    'ğŸ˜–': ' negative ', 'ğŸ™': ' positive ', 'ğŸš«': ' positive ',  'â¤ï¸': ' positive ', 'ğŸ¤': ' positive ', ":'>": ' positive ', 'ğŸŒ·': ' positive ',
    '=))' : ' positive ', ':))' : ' positive ', ':(' : ' negative ', ':)': ' positive ', 'âˆ©_âˆ©': ' positive ', '^^': ' positive ', ':|': ' negative ', '^=^': ' positive ',
    '(à¹‘>â—¡<à¹‘)': ' positive ', 'ã„Ÿ(ï¿£â–½ï¿£ã„Ÿ)': ' positive ', ':3': ' positive ', ':v': ' positive ', ':((': ' negative ', '=}}': ' positive ', 'T^T': ' negative ', '<3': ' positive ',
    '^_^' : ' positive ', '1 sao': ' negative ', '1*': ' negative ', '2 sao': ' negative ', '2sao': ' negative ', '2*': ' negative ', '3 sao': ' positive ', '3*': ' positive ', '3sao': ' positive ',
    '4 sao': ' positive ', '4*': ' positive ', '4sao' : ' positive ', '5 sao': ' positive ', '5*': ' positive ', 'ğŸ’®': ' positive ', 'ğŸ˜ƒ': ' positive ', 'ğŸ’': ' positive ',
    'vcl': ' negative ', 'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ', 'ğŸ†—': ' positive ', 'ğŸ’–': ' positive ',
    ' lol ': ' negative ',' cc ': ' negative ','huhu': ' negative ', 't^t': ' negative ', 'ğŸ’—': ' positive ', 'ğŸ˜™': ' positive ', 'ğŸ™‚': ' negative ',
    'ğŸ’›': ' positive ', 'ğŸ’': ' positive ', '-.-': ' negative ', 'okÃª': ' ok ', 'ğŸŒ¸': ' positive  ', 'â£':  ' positive ', 'ğŸ¤ª': ' positive ', 'ğŸ¤™ğŸ»': ' positive ', 'â™¡': ' positive ',
    '5sao': ' positive ', '1sao': ' negative '
}

#vá»‹ trÃ­ dáº¥u thanh á»Ÿ viá»‡t nam chá»‰ tá»“n táº¡i khÃ´ng nháº¥t trÃ­ vá»›i cÃ¡c tá»• há»£p oa, oe, ua, ue, uy cháº³ng háº¡n nhÆ°
# Há»a, hÃ²e, há»§y, qá»§a, qá»§e, quÃ½, hoáº¡, hoÃ¨, quáº£, quáº», quÃ½... thÃ¬ pháº£i Ä‘iá»u chá»‰nh vá» Ä‘Ãºng dáº¥u
# há»a
vitridauthanh = {
    'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',
    'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ', 'á»§a': 'uáº£'
}
rightword = {
    ' kb ': ' khÃ´ng ', 'sp': ' sáº£n pháº©m ', 'ship': ' váº­n chuyá»ƒn ', 'Ä‘c': ' Ä‘Æ°á»£c ', 'dc': ' Ä‘Æ°á»£c ', 'dx': ' Ä‘Æ°á»£c ',
    'shop': ' cá»­a hÃ ng ', 'tks': ' cáº£m Æ¡n ', 'thank': ' cáº£m Æ¡n ', 'thanks': ' cáº£m Æ¡n ', 'tl': ' tráº£ lá»i ',
    'rep': ' tráº£ lá»i ', 'delivery': ' váº­n chuyá»ƒn ', 'poor ': ' kÃ©m ', 'product ': ' sáº£n pháº©m ', 'quality': ' cháº¥t lÆ°á»£ng ',
    'hk': ' khÃ´ng ', 'wÃ¡': ' quÃ¡ ', 'Ä‘t': ' Ä‘iá»‡n thoáº¡i ', 'kq': ' káº¿t quáº£ ', 'mk': ' mÃ¬nh ', 'check': ' kiá»ƒm tra ',
    'update': ' cáº­p nháº­t ', 'mn': ' má»i ngÆ°á»i ', 'feedback': ' pháº£n há»•i ', 'wa': ' quÃ¡ ', 'app': ' á»©ng dá»¥ng ',
    'mik': ' mÃ¬nh ', 'ntn': ' nhÆ° tháº¿ nÃ y ', 'size': ' kÃ­ch cá»¡ ', ' j ': ' gÃ¬ ', ' ji ': ' gÃ¬ ', 'Ä‘k': ' Ä‘Æ°á»£c ',
    'authentic': ' chÃ­nh hÃ£ng ', 'auth': ' chÃ­nh hÃ£ng', 'fake': ' giáº£ máº¡o ', 'pack': ' Ä‘Ã³ng gÃ³i ', 'packing': ' Ä‘Ã³ng gÃ³i ', 'order': ' Ä‘áº·t hÃ ng ',
    'cháº¥t lg': ' cháº¥t lÆ°á»£ng ', 'okie': ' ok ', 'oke': ' ok ', 'oki': ' ok ', ' m ': ' mÃ¬nh ', 'sd': ' sá»­ dá»¥ng ', 'fb ': ' facebook ',
    'ib': ' nháº¯n tin ', 'time': ' thá»i gian ', 'dt': ' Ä‘iá»‡n thoáº¡i ',
    'Ä‘t ': ' Ä‘iá»‡n thoáº¡i ', 'thik': ' thÃ­ch ', 'mjh': ' mÃ¬nh ', 'okey': ' ok ', 'sz ': ' kÃ­ch cá»¡ ', ' very ': ' ráº¥t ', 'hsd': ' háº¡n sá»­ dá»¥ng ', 'qÃ¡': ' quÃ¡ ',
    'bt': ' bÃ¬nh thÆ°á»ng ', 'money': ' tiá»n ', 'value': ' giÃ¡ trá»‹ ', 'fast': ' nhanh ', 'excelent': ' tá»‘t ',
    ' k ': ' khÃ´ng ',' kh ':' khÃ´ng ', 'kÃ´':' khÃ´ng ', 'hok':' khÃ´ng ',' kp ': ' khÃ´ng pháº£i ',' kÃ´ ': ' khÃ´ng ', 'ko': ' khÃ´ng ',
    'khong': ' khÃ´ng ', ' hok ': ' khÃ´ng ', 'inbox': ' nháº¯n tin ',
}

correctmapping = {**emoji, **rightword}

# string = r"Sáº¢N PHáº¨Mhttp://tnews.vn/index.php?threads/43/Review giá»›i thiá»‡u :Giá»›i thiá»‡u kÃ¨m bá»‹ bá»Ÿi cáº£ Lá»™c  cÃ¡i   cáº§n   cÃ ng <  =, >, ?, @"
# print(string.isalpha())
# url = 'https?:\/\/[^\s]*'
# match_url1 = re.findall(url, string)
# if match_url1:
#     print(match_url1)
# string1 = "ÄÃ´ng thÃ¬ ráº¥t Ä‘áº¹p trai nÃªn lÃ  háº¥p dáº«n he he"
# # print(string1.lower())
# a = ViTokenizer.tokenize(string1)
# print(a)
# a = RelgularExpression(string)
# a = RemoveStopword(a)
# for row in a.split():
#     if row.isalpha() is False:
#         print(row)

# print(review)

#todo tÃ¬m emoji trong data
def NonCharacter(data):
    notcharacter = '[^A-Za-z0-9 áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ".,]+'
    for row in data:
        match = re.findall(notcharacter, row)
        if match:
            print(match)

#
# NonCharacter(data)
# count = 0
# a = list()
# for row in data:
#     tmp = list()
#     for i in row:
#         if i in correctmapping:
#             tmp.append(correctmapping[i])
#             # count = count + 1
#     a.append(tmp)
# print(count)

# with open('emoji.txt', 'w', encoding="utf-8") as f:
#     for item in a:
#         f.write("%s\n" % item)

# string1 = "Uá»‘ng ráº¥t ngon Giao hÃ ng nhanh Cháº¥t lÆ°á»£ng sáº£n pháº©m tuyá»‡t vá»i ....................ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹......................"

# print(a)
# print(a)

# string1 = string1.strip().split()
# for i in string1:
#     print(i)
#     if i in correctmapping:
#         print(i)

# string1 = underthesea.word_tokenize(string1)
# print(string1)

#todo thay tháº¿ cÃ¡c emoji vÃ  cÃ¡c tá»« viáº¿t táº¯t
def ReplaceWithCorrectmapping(row):
    keys = correctmapping.keys()
    for i in keys:
        if i in row:
            # print(i)
            row = row.replace(i, correctmapping[i])
    return row

#todo thay tháº¿ cÃ¡c vá»‹ trÃ­ dáº¥u thanh Ä‘áº·c biá»‡t
def ReplaceDauThanh(row):
    keys = vitridauthanh.keys()
    for i in keys:
        if i in row:
            # print(i)
            row = row.replace(i, vitridauthanh[i])
    return row

#todo dem emoji positive and negative
def PosNegCount(data):
    pos = "positive"
    neg = "negative"
    num_positive = 0
    num_negative = 0

    matchpos = re.findall(pos, data)
    matchneg = re.findall(neg, data)
    if matchpos:
        num_positive = matchpos.__len__()

    if matchneg:
        num_negative = matchneg.__len__()

    return num_positive, num_negative

#todo xá»­ lÃ½ data vá»›i cÃ¡c function
def Preprocessing(frame):
    pos, neg, not_dict = SentimentFile()
    new_review = list()
    uni_new_review = list()
    # daugach = re.compile(r"[_]")
    a = 0
    # customfeature = list()
    # customfeature = pd.DataFrame(columns=['count_not', 'count_neg', 'count_pos', 'num_words', 'num_punc' , 'emo_positive', 'emo_negative'])
    for row in frame:
        row = row.lower()
        row = ReplaceWithCorrectmapping(row) # kp lá»—i tá»« Ä‘Ã¢y
        row = ReplaceDauThanh(row) #kp lá»—i tá»« Ä‘Ã¢y
        row, numpunc = RelgularExpression(row) #kp lá»—i tá»« Ä‘Ã¢y
        #row = RemovePunc(row) #RemovePunc cÃ³ váº¥n Ä‘á»
        row = re.sub(r'(\D)\1+', r'\1', row)
        row = re.sub(r'\xa0', r' ', row)
        row = re.sub(r'\n', r' ', row)
        row = row.strip()
        row = row.split()
        row1 = list()
        for word in row:
            if (word.isdigit() == False):
                row1.append(word)
        row = ' '.join(row1)
        # row = ' '.join(row.split())
        # row = re.sub(daugach, ' ', row)

        reviewfinal = []
        #tokenize
        review = list()
        review1 = word_tokenize(row)
        for i in review1:
            i_ = "_".join(i.split())
            review.append(i_)
        # review = (ViTokenizer.tokenize(row)).split(" ")
        # num_words = len(review)
        review, smallfeature = neg_pos(review, not_dict, neg, pos)
        # print(review)
        #remove stopword
        for word in review:
            if word not in stopword:
                if (("_" in word) or (word.isalpha() == True)):
                    reviewfinal.append(word)

        # smallfeature.append(num_words)
        # smallfeature.append(numpunc)
        review1 = ' '.join(reviewfinal)
        new_review.append(review1)
        # num_positive, num_negative = PosNegCount(review1)
        # smallfeature.append(num_positive)
        # smallfeature.append(num_negative)
        #data ko dau
        unirow = unidecode.unidecode(' '.join(reviewfinal))
        uni_new_review.append(unirow)
        a = a + 1
        # print(smallfeature)
        # customfeature.append(smallfeature)
    # with open('data.txt', 'w', encoding="utf-8") as f:
    #     for item in new_review:
    #         f.write("%s\n" % item)
        # custom = pd.DataFrame(customfeature,
        #                       columns=['count_not', 'count_neg', 'count_pos', 'num_words', 'num_punc', 'emo_positive',
        #                                'emo_negative']) # print(type(smallfeature))

    return new_review, uni_new_review


# # print(data[347])
# data,unicodedata = Preprocessing(data)
# datafinal = data + unicodedata
# labelfinal = label + label

# with open('data_final.txt', 'w', encoding="utf-8") as f:
#      for item in data:
#           f.write("%s\n" % item)
