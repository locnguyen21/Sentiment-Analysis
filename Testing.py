# from Preprocessing import *
# from Readfile import *
# from nltk.tokenize import word_tokenize
from Ftext import *
from RNN import *
from SVM import *
from TFIDF import *
#from Doc2vec import *
import pickle
import numpy as np
# from Test_RNN_Fasttext import *
from gensim.models.fasttext import FastText
#from tensorflow import keras
#import tensorflow as tf
#import gensim.models as g
#from keras.models import model_from_json,load_model
# from keras.utils import CustomObjectScope
# from keras.initializers import glorot_unifo
# from sklearn.preprocessing import StandardScaler


# label, id_list, indexxuoc, indexid, df, data = readfile()
# new_review, uni_new_review= Preprocessing(data)
# # # # # with open('data.txt', 'w', encoding="utf-8") as f:
# # # # #     for item in new_review:
# # # # #         f.write("%s\n" % item)
# new_review = new_review + uni_new_review
# label = label + label

# feature_name = custom.columns
# custom = custom.values
# custom = custom.tolist() + custom.tolist()
# custom = pd.DataFrame(custom, columns= feature_name)
#todo word_token for fasttext
#Fasttext and RNN model

# reviews = list()
# for review in new_review:
#     review = word_tokenize(review)
#     reviews.append(review)
# dataf = BuildFastText(reviews, label)
# RNNmodel(dataf, label)


# #TFIDF and SVM model
# vectordata = Tfidf(new_review)
# # # # # # # data = pd.concat([tframe, custom], axis=1)
# # # # # # # # # print(new_review[17244])
# # # # # # # vectordata = StandardScaler().fit_transform(data.values)
# SVM(vectordata, label)

# todo Doc2Vec and RNN model
# vectordata  = Doc2vec(new_review)
# RNNmodel(vectordata, label)

# print('read file:')
data_new = ["T·ªá. Gi√†y s·ª©t ch·ªâ h·ªôp r√°ch. Th·ªùi gian giao h√†ng r·∫•t ch·∫≠m",
            "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m !!!To√†n b·ªã s·ªùn ch·ªâ !!! Kh√¥ng ƒë√∫ng v·ªõi m√¥ t·∫£ c·ªßa s·∫£n ph·∫©m !!!",
            "H√†ng l·ªói. S·∫°c h·ªèng. Lm ƒÉn qu√° ch√°n R·∫•t kh√¥ng ƒë√°ng ti·ªÅn R·∫•t kh√¥ng ƒë√°ng ti·ªÅn",
            "S·∫£n ph·∫©m g·ªôi r·∫•t l√† c·ª©ng t√≥c.m√πi th∆°m nh∆∞ ki·ªÉu ho√° ch·∫•t. Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m.",
            "c·ª±c kh√¥ng ·ªïn üôÅüôÅüôÅ, ch·∫•t l∆∞·ª£ng k√©m",
            "kh√¥ng t·ªët nh∆∞ k·ª≥ v·ªçng",
            'h√†ng kh√¥ng t·ªët',
            "K√©m ch·∫•t l∆∞·ª£ng",
            'h√†ng d·ªüm',
            'H√†ng t·ªët. ƒê√≥ng g√≥i sp k·ªπ. Nh√¢n vi√™n t∆∞ v·∫•n n√™n c√≥ th√°i ƒë·ªô d·ªÖ ch·ªãu h∆°n',
            "·ªïn",
            "r·∫•t ·ªïn",
            "ch·∫•t l∆∞·ª£ng k√©m, pin h·∫øt nhanh",
            "·ªêp n√†o c≈©ng ƒë·∫πp h·∫øt th√≠ch c·ª±c lu√¥nnnnn",
            "H√†ng r·∫•t t·ªët",
            "b·ª±c m√¨nh, ch√°n",
            "Shop ph·ª•c v·ª• qu√° t·ªá. Mua pho mai s·ª£i m√† b√°n pho mai l√°t. L√°t pho mai c√≥ m√πi v√† m√≥p m√©o",
            "ƒê√£ nh·∫≠n ƒëc 1 c√°i. Thanks shop ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn Shop ph·ª•c v·ª• r·∫•t t·ªët",
            "S·∫£n ph·∫©m kh√¥ng bi·∫øt c√≥ t·ªët hay kh√¥ng ?v√¨ mu·ªën ƒë√°nh gi√° ph·∫£i c√≥ th·ªùi gian ki·ªÉm tra  ƒë√£ ki·ªÉm tra ch·∫•t l∆∞·ª£ng nh∆∞ con cac",
            "H√†ng nh√°i ·ªçp √† ·ªçp ·∫πp",
            "L·∫•y size t·ª´ 12/14 kg m√† m·∫∑c ko v·ª´a .ch√¢t l∆∞·ª£ng th√¨ ok nh∆∞ng m·∫∑c ch·∫≠t", #1
            "∆∞ng c·ª±c k·ª≥.",
            "H√†ng qu√° ch√°n m·∫•y b·ªô b√© zai kh√°c v·ªõi h√¨nh shop ƒëƒÉng b√°n kh√¥ng  c√≥ b·ªô n√†o m√¨nh ch·ªçn gi·ªëng nh∆∞ l√∫c ƒë·∫∑t"]
# data_label = np.array([1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1, 1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1])
# data, unidata = Preprocessing(data_new)
# data = data + unidata

def SVM_Tfidf(data):
    tfidf_model = pickle.load(open("C://Users//locco//PycharmProjects//SentimentAnalysis//Model//tfidf.pickle", "rb"))
    vector = tfidf_model.transform(data)
    vectordata = vector.todense()
    # vectordata = StandardScaler().fit_transform(vectordata)
    # print(vectordata.shape)
    clf = pickle.load(open("C://Users//locco//PycharmProjects//SentimentAnalysis//Model//SVM_model.sav", "rb"))
    result = clf.predict(vectordata)
    return result

# result = SVM_Tfidf(data)
# # # result = RNN_Fasttext(data)
# print(result)
# print("K·∫øt qu·∫£: ")
# for i in range(0, len(result)):
#     if (result[i] != data_label[i]):
#         print(data[i])
#         print(result[i])
# Evaluate_Model(result, data_label)


# result = RNN_Fasttext(data)
# print(result)
# Evaluate_Model(result, data_label)
#todo doc2vec
# doc2vecfile="Model/doc2vec_model.bin"  #point to downloaded pre-trained doc2vec model
# #load model
# m = g.Doc2Vec.load(doc2vecfile)
# token = "ch·∫•t l∆∞·ª£ng k√©m, pin h·∫øt nhanh"
# vector = m.infer_vector(word_tokenize(token))
# print(vector)
# token = list()
# for i in data_new:
#     vector2 = word_tokenize(i)
#     token.append(vector2)
# vectors = m.infer_vector([token[0]])
# print(vector)
# print(len(vector))
# print(vector2)
#
# print(type(vector))
# test = np.reshape(vector,(1,len(vector),1))
# print(test.shape)
# print(type(load_model1))
# labels_test = load_model1.predict(test)
# # print(reviews[2])
# a = labels_test.tolist()
# if(a[0][0] <0.5):
#     print(0)
# if(a[0][0] >= 0.5):
#     print(1)


# # if (a[0] >= 0.5):
#     print(1)
# if (a[0] < 0.5):
#     print(0)


# labels = []
# for i in labels_test:
#     if (i >= 0.5):
#         a = 1
#         labels.append(a)
#     else:
#         a = 0
#         labels.append(a)
# labels = np.asarray(labels)
# # print(type(labels))
# print(labels)
