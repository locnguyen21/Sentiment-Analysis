# from Preprocessing import *
# from Readfile import *
from nltk.tokenize import word_tokenize
from Ftext import *
from RNN import *
# from SVM import *
# from TFIDF import *
# from Doc2vec import *
import pickle
import numpy as np
from gensim.models.fasttext import FastText
# from tensorflow import keras
import tensorflow as tf
# import gensim.models as g
# from keras.models import model_from_json,load_model
# from keras.utils import CustomObjectScope
# from keras.initializers import glorot_unifo
# from sklearn.preprocessing import StandardScaler


# label, id_list, indexxuoc, indexid, df, data = readfile()
# new_review, uni_new_review= Preprocessing(data)
# new_review = new_review + uni_new_review
# label = label + label

# todo word_token for fasttext
# Fasttext and RNN model
# reviews = list()
# for review in new_review:
#     review = word_tokenize(review)
#     reviews.append(review)
# dataf = BuildFastText(reviews, label)
# RNNmodel(dataf, label)

data_new = ["Tá»‡. GiÃ y sá»©t chá»‰ há»™p rÃ¡ch. Thá»i gian giao hÃ ng ráº¥t cháº­m",
            "Cháº¥t lÆ°á»£ng sáº£n pháº©m ráº¥t kÃ©m !!!ToÃ n bá»‹ sá»n chá»‰ !!! KhÃ´ng Ä‘Ãºng vá»›i mÃ´ táº£ cá»§a sáº£n pháº©m !!!",
            "HÃ ng lá»—i. Sáº¡c há»ng. Lm Äƒn quÃ¡ chÃ¡n Ráº¥t khÃ´ng Ä‘Ã¡ng tiá»n Ráº¥t khÃ´ng Ä‘Ã¡ng tiá»n",
            "Sáº£n pháº©m gá»™i ráº¥t lÃ  cá»©ng tÃ³c.mÃ¹i thÆ¡m nhÆ° kiá»ƒu hoÃ¡ cháº¥t. Cháº¥t lÆ°á»£ng sáº£n pháº©m ráº¥t kÃ©m.",
            "cá»±c khÃ´ng á»•n ðŸ™ðŸ™ðŸ™, cháº¥t lÆ°á»£ng kÃ©m",
            "khÃ´ng tá»‘t nhÆ° ká»³ vá»ng",
            'hÃ ng dá»Ÿm',
            'hÃ ng khÃ´ng tá»‘t',
            "KÃ©m cháº¥t lÆ°á»£ng",
            'HÃ ng tá»‘t. ÄÃ³ng gÃ³i sp ká»¹. NhÃ¢n viÃªn tÆ° váº¥n nÃªn cÃ³ thÃ¡i Ä‘á»™ dá»… chá»‹u hÆ¡n',
            "á»•n",
            "ráº¥t á»•n",
            "cháº¥t lÆ°á»£ng kÃ©m, pin háº¿t nhanh",
            "á»p nÃ o cÅ©ng Ä‘áº¹p háº¿t thÃ­ch cá»±c luÃ´nnnnn",
            "HÃ ng ráº¥t tá»‘t",
            "bá»±c mÃ¬nh, chÃ¡n",
            "Shop phá»¥c vá»¥ quÃ¡ tá»‡. Mua pho mai sá»£i mÃ  bÃ¡n pho mai lÃ¡t. LÃ¡t pho mai cÃ³ mÃ¹i vÃ  mÃ³p mÃ©o",
            "ÄÃ£ nháº­n Ä‘c 1 cÃ¡i. Thanks shop cháº¥t lÆ°á»£ng sáº£n pháº©m tuyá»‡t vá»i ÄÃ³ng gÃ³i sáº£n pháº©m ráº¥t Ä‘áº¹p vÃ  cháº¯c cháº¯n Shop phá»¥c vá»¥ ráº¥t tá»‘t",
            "Sáº£n pháº©m khÃ´ng biáº¿t cÃ³ tá»‘t hay khÃ´ng ?vÃ¬ muá»‘n Ä‘Ã¡nh giÃ¡ pháº£i cÃ³ thá»i gian kiá»ƒm tra  Ä‘Ã£ kiá»ƒm tra cháº¥t lÆ°á»£ng nhÆ° con cac",
            "HÃ ng nhÃ¡i á»p Ã  á»p áº¹p",
            "Láº¥y size tá»« 12/14 kg mÃ  máº·c ko vá»«a .chÃ¢t lÆ°á»£ng thÃ¬ ok nhÆ°ng máº·c cháº­t", #1
            "Æ°ng cá»±c ká»³.",
            "HÃ ng quÃ¡ chÃ¡n máº¥y bá»™ bÃ© zai khÃ¡c vá»›i hÃ¬nh shop Ä‘Äƒng bÃ¡n khÃ´ng  cÃ³ bá»™ nÃ o mÃ¬nh chá»n giá»‘ng nhÆ° lÃºc Ä‘áº·t"]
# data_label = np.array([1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1,   1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1])
# data, unidata = Preprocessing(data_new)
# data = data + unidata


def RNN_Fasttext(data):
    # data, unidata = Preprocessing(data_new)
    reviews = list()
    for review in data:
        review = word_tokenize(review)
        reviews.append(review)

    print(reviews)
# print(len(data))
    model = FastText.load("C:/Users/locco/PycharmProjects/SentimentAnalysis/Model/fasttext.bin")
# # print(reviews)
# # review_vectors = model.wv[reviews]
#
# a = 0
# b = 0
    dataf = list()
#
    for review in reviews:
    # print(a)
        len_sen = len(review)
    # print(type(len_sen))
        try:
            vectors = model.wv[review]
        # print(type(vectors))
            sumvec = 0
            for i in range(0, len_sen):
                sumvec = sumvec + vectors[i]
            sumvec = sumvec / len_sen
            #np.array
            # print(type(sumvec))
        # print(type(sumvec))
        # print(sumvec)
            dataf.append(sumvec)
        except:
            sumvec = 0
        # print(a)
        # print(review)
        #     b = b + 1
            dataf.append(sumvec)
    #
    # # print(dataf)
    dataf = np.array(dataf)
    print(dataf.shape)
    # print(len(dataf))
    json_file = open('C://Users//locco//PycharmProjects//SentimentAnalysis//Model//RNN_model_ftext.json','r')
    loaded_model_json = json_file.read()
    json_file.close()
    load_model1 = tf.keras.models.model_from_json(loaded_model_json)
# # # with CustomObjectScope({'GlorotUniform': glorot_uniform()}):
# # #     model = load_model('imdb_mlp_model.h5')
# # #load weights vaÌ€o model
    load_model1.load_weights('C://Users//locco//PycharmProjects//SentimentAnalysis//Model//RNN_model_ftext.h5')
    dataf = np.reshape(dataf, (dataf.shape[0], dataf.shape[1], 1))
    result = load_model1.predict(dataf)
    # print(result)
    labels = []
    for i in result:
        if (i >= 0.5):
            a = 1
            labels.append(a)
        else:
            a = 0
            labels.append(a)
    labels = np.asarray(labels)
    return labels

# labels = RNN_Fasttext(data)
# print(labels)
# Evaluate_Model(labels.tolist(), data_label)
#
# for i in range(0, len(labels)):
#     if (labels[i] != data_label[i]):
#         print(data[i])
#         print(labels[i])