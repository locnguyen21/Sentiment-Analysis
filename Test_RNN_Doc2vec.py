from Preprocessing import *
from Readfile import *
import underthesea
from nltk.tokenize import word_tokenize
from Ftext import *
from RNN import *
from SVM import *
from TFIDF import *
from Doc2vec import *
import pickle
import numpy as np
from gensim.models.fasttext import FastText
from tensorflow import keras
import tensorflow as tf
import gensim.models as g
from keras.models import model_from_json,load_model



# label, id_list, indexxuoc, indexid, df, data = readfile()
# new_review, uni_new_review= Preprocessing(data)
# new_review = new_review + uni_new_review
# label = label + label
# #
# # # todo Doc2Vec and RNN model
# vectordata  = Doc2vec(new_review)
# RNNmodel(vectordata, label)

data_new = ["Tá»‡. GiÃ y sá»©t chá»‰ há»™p rÃ¡ch. Thá»i gian giao hÃ ng ráº¥t cháº­m",
            "Cháº¥t lÆ°á»£ng sáº£n pháº©m ráº¥t kÃ©m !!!ToÃ n bá»‹ sá»n chá»‰ !!! KhÃ´ng Ä‘Ãºng vá»›i mÃ´ táº£ cá»§a sáº£n pháº©m !!!",
            "HÃ ng lá»—i. Sáº¡c há»ng. Lm Äƒn quÃ¡ chÃ¡n Ráº¥t khÃ´ng Ä‘Ã¡ng tiá»n Ráº¥t khÃ´ng Ä‘Ã¡ng tiá»n",
            "Sáº£n pháº©m gá»™i ráº¥t lÃ  cá»©ng tÃ³c.mÃ¹i thÆ¡m nhÆ° kiá»ƒu hoÃ¡ cháº¥t. Cháº¥t lÆ°á»£ng sáº£n pháº©m ráº¥t kÃ©m.",
            "cá»±c khÃ´ng á»•n ðŸ™ðŸ™ðŸ™, cháº¥t lÆ°á»£ng kÃ©m",
            "khÃ´ng tá»‘t nhÆ° ká»³ vá»ng",
            'hÃ ng dá»Ÿm',
            'hÃ ng khÃ´ng tá»‘t',
            "KÃ©m cháº¥t lÆ°á»£ng",
            'HÃ ng tá»‘t. ÄÃ³ng gÃ³i sp ká»¹. NhÃ¢n viÃªn tÆ° váº¥n nÃªn cÃ³ thÃ¡i Ä‘á»™ dá»… chá»‹u hÆ¡n',
            "á»•n",
            "ráº¥t á»•n",
            "cháº¥t lÆ°á»£ng kÃ©m, pin háº¿t nhanh",
            "á»p nÃ o cÅ©ng Ä‘áº¹p háº¿t thÃ­ch cá»±c luÃ´nnnnn",
            "HÃ ng ráº¥t tá»‘t",
            "bá»±c mÃ¬nh, chÃ¡n",
            "Shop phá»¥c vá»¥ quÃ¡ tá»‡. Mua pho mai sá»£i mÃ  bÃ¡n pho mai lÃ¡t. LÃ¡t pho mai cÃ³ mÃ¹i vÃ  mÃ³p mÃ©o",
            "ÄÃ£ nháº­n Ä‘c 1 cÃ¡i. Thanks shop cháº¥t lÆ°á»£ng sáº£n pháº©m tuyá»‡t vá»i ÄÃ³ng gÃ³i sáº£n pháº©m ráº¥t Ä‘áº¹p vÃ  cháº¯c cháº¯n Shop phá»¥c vá»¥ ráº¥t tá»‘t",
            "Sáº£n pháº©m khÃ´ng biáº¿t cÃ³ tá»‘t hay khÃ´ng ?vÃ¬ muá»‘n Ä‘Ã¡nh giÃ¡ pháº£i cÃ³ thá»i gian kiá»ƒm tra  Ä‘Ã£ kiá»ƒm tra cháº¥t lÆ°á»£ng nhÆ° con cac",
            "HÃ ng nhÃ¡i á»p Ã  á»p áº¹p",
            "Láº¥y size tá»« 12/14 kg mÃ  máº·c ko vá»«a .chÃ¢t lÆ°á»£ng thÃ¬ ok nhÆ°ng máº·c cháº­t", #1
            "Æ°ng cá»±c ká»³.",
            "HÃ ng quÃ¡ chÃ¡n máº¥y bá»™ bÃ© zai khÃ¡c vá»›i hÃ¬nh shop Ä‘Äƒng bÃ¡n khÃ´ng  cÃ³ bá»™ nÃ o mÃ¬nh chá»n giá»‘ng nhÆ° lÃºc Ä‘áº·t"]
data_label = np.array([1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1, 1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1])
data, unidata = Preprocessing(data_new)
data = data + unidata

doc2vecfile="Model/doc2vec_model.bin"  #point to downloaded pre-trained doc2vec model
#load model
m = g.Doc2Vec.load(doc2vecfile)

json_file = open('Model/RNN_model_doc2vec.json','r')
loaded_model_json = json_file.read()
json_file.close()
load_model1 = tf.keras.models.model_from_json(loaded_model_json)
# # # with CustomObjectScope({'GlorotUniform': glorot_uniform()}):
# # #     model = load_model('imdb_mlp_model.h5')
# # #load weights vaÌ€o model
load_model1.load_weights('Model/RNN_model_doc2vec.h5')

token = list()
for i in data:
    vector2 = word_tokenize(i)
    token.append(m.infer_vector(vector2))
token = np.array(token)
print(token.shape)
dataf = np.reshape(token, (token.shape[0], token.shape[1], 1))
result = load_model1.predict(dataf)
print(type(result))
labels = []
for i in result:
    if (i >= 0.5):
        a = 1
        labels.append(a)
    else:
        a = 0
        labels.append(a)
labels = np.asarray(labels)
Evaluate_Model(labels.tolist(), data_label)

for i in range(0, len(labels)):
    if (labels[i] != data_label[i]):
        print(data[i])
        print(labels[i])